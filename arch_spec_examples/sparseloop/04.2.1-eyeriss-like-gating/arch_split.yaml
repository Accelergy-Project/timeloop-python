architecture:
  version: 0.4
  nodes:
  - !Container
    name: eyeriss_168
    attributes:
      technology: "45nm"

  - !Component
    name: DRAM
    class: DRAM
    attributes:
      width: 64
      block_size: 4
      datawidth: 16
      metadata_storage_width: 60
      metadata_datawidth: 5
      metadata_block_size: 12

  - !Component
    name: shared_glb
    class: storage
    subclass: smartbuffer_metadata
    attributes:
      data_storage_depth: 12800
      data_storage_width: 64
      block_size: 4
      n_banks: 1
      read_bandwidth: 16
      write_bandwidth: 16
      metadata_storage_depth: 0
      metadata_storage_width: 0
      datawidth: 16
      metadata_block_size: 1
      metadata_datawidth: 0
      metadata_counter_width: 9
      # Note that the two diretives below are describing 
      # whether this level can compress and decompress data transfers
      # from itself to its parent (not child)
      decompression_supported: true  # from DRAM decompression
      compression_supported: true    # to DRAM compression
    constraints:
      dataspace: {keep: [Inputs, Outputs], bypass: [Weights]}

  - !Container # Each column of PEs produces a different psum row
    name: PE_column
    spatial: {meshX: 14}
    constraints:
      spatial:
        permutation: [N, C, P, R, S, Q, M]
        factors: [N=1, C=1, P=1, R=1, S=1]
        split: 7

  - !Container # Each PE in the column receives a different filter row
    name: PE
    spatial: {meshY: 12}
    constraints:
      spatial:
        split: 4
        permutation: [N, P, Q, R, S, C, M]
        factors: [N=1, P=1, Q=1, R=1]

  - !Parallel # Input/Output/Weight scratchpads in parallel
    nodes:
    - !Component
      name: ifmap_spad
      class: storage
      subclass: smartbuffer
      attributes:
        data_storage_depth: 12
        data_storage_width: 17 # actual data + 1bit mask
        n_banks: 1
        datawidth: 17
        read_bandwidth: 2
        write_bandwidth: 2
      constraints:
        dataspace: {keep: [Inputs]}
        temporal:
          permutation: [N, M, C, P, Q, R, S]
          factors: [N=1, M=1, C=1, P=1, Q=1, R=1, S=1]

    - !Component
      name: weights_spad
      class: storage
      subclass: smartbuffer
      attributes:
        data_storage_depth: 224
        data_storage_width: 16
        n_banks: 1
        datawidth: 16
        read_bandwidth: 2
        write_bandwidth: 2
      constraints:
        dataspace: {keep: [Weights]}
        temporal:
          permutation: [N, M, P, Q, S, C, R]
          factors: [N=1, M=1, P=1, Q=1, S=1]

    - !Component
      name: psum_spad
      class: storage
      subclass: smartbuffer
      attributes:
        data_storage_depth: 24
        data_storage_width: 16
        n_banks: 1
        datawidth: 16
      constraints:
        dataspace: {keep: [Outputs]}
        temporal:
          permutation: [N, C, P, Q, R, S, M] 
          factors: [N=1, C=1, R=1, S=1, P=1, Q=1]

  - !Component
    name: MACs
    class: compute
    subclass: mac
    attributes:
      datawidth: 16


# eyeriss v1 optimization features
#  - DRAM compression for Inputs and Weights
#    - for each tile sent from DRAM to GLB, the dimensions are flattened together, thus flattened_rankIDs include all dimensions
#    - we carry a top level UOP flag for each compressed GLB tile to indicate the order in which the compressed tile is sent from DRAM
#  - gating on weights scratchpad based on input activation's sparsity
#  - note that the bitmask metadata associated with the ifmap scratchpad is not here because the metadata did not
#    allow ifmap accesses to be saved, which is what conventionally what metadata will help to achieve, so we model
#    it as an extra bit in the datawidth instead of as a metadata
sparse_optimizations:
  version: 0.4
  targets:
    - target: DRAM
      representation_format:
        data_spaces:
          - name: Inputs
            ranks:
              - format: UOP
                flattened_rankIDs: [ [R, S, P, C, M, N, Q] ]
              - format: RLE
                flattened_rankIDs: [ [R, S, P, C, M, N, Q] ]

          - name: Outputs
            ranks:
              - format: UOP
                flattened_rankIDs: [ [P, M, N, Q] ]
              - format: RLE
                flattened_rankIDs: [ [P, M, N, Q] ]

    - target: weights_spad
      action_optimization:
        - type: gating
          options:
            - target: Weights
              condition_on: [ Inputs ]

mapspace_constraints:
  version: 0.4
  targets:
    # intuitive optimization for row stationary
    # -> process a row/col of the output before going to the next one
    - target: shared_glb
      type: temporal
      permutation: [Q, R, S, C, P, N, M] 
      factors: [Q=1, R=1, S=1, P=0]
    # intuitive optimization to reduce map space size
    - target: DRAM
      type: temporal
      permutation: [R, S, P, C, M, N, Q]
      factors: [R=1, S=1, P=1]

mapping:
  # certain buffer only stores certain datatypes
  - target: DRAM
    type: temporal
    permutation: [R, S, P, C, M, N, Q]
    factors: [R=1, S=1, P=1, C=64, M=6, N=1, Q=1]
  - target: DRAM
    type: dataspace
    keep: [Weights, Inputs, Outputs]


  # only allow fanout of M, Q out from glb
  - target: PE_column
    type: spatial
    split: 7
    permutation: [N, C, P, R, S, Q, M]
    factors: [N=1, C=1, P=1, R=1, S=1, Q=0, M=1]
  - target: shared_glb
    type: temporal
    permutation: [Q, R, S, C, P, N, M] 
    factors: [Q=1, R=1, S=1, P=0, C=1, N=4, M=1]
  - target: shared_glb
    type: dataspace
    bypass: [Weights]
    keep: [Inputs, Outputs]

  - target: PE
    type: spatial
    split: 4
    permutation: [N, P, Q, R, S, C, M]
    factors: [N=1, P=1, Q=1, R=1, S=0, C=1, M=4]

  - target: ifmap_spad
    type: dataspace
    bypass: [Weights, Outputs]
    keep: [Inputs]
  - target: ifmap_spad
    type: temporal
    permutation: [N, M, C, P, Q, R, S]
    factors: [N=1, M=1, C=1, P=1, Q=1, R=1, S=1]


  # row stationary -> 1 row at a time
  - target: weights_spad
    type: temporal
    permutation: [N, M, P, Q, S, C, R]
    factors: [N=1, M=1, P=1, Q=1, S=1, R=0, C=4]
  - target: weights_spad
    type: dataspace
    bypass: [Inputs, Outputs]
    keep: [Weights]

  # one ofmap position but of different output channels
  - target: psum_spad
    type: temporal
    permutation: [N, C, P, Q, R, S, M] 
    factors: [N=1, C=1, R=1, S=1, P=1, Q=1, M=16]
  - target: psum_spad
    type: dataspace
    bypass: [Inputs, Weights]
    keep: [Outputs]