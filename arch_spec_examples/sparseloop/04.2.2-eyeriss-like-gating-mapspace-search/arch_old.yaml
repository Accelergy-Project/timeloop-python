architecture:
  version: 0.4
  subtree:
    - name: eyeriss_168
      required_actions: [read, write, update, leak]
      attributes:
        technology: -1
      local:
        - name: DRAM
          class: DRAM
          required_actions: [read, write, update, leak]
          attributes:
            technology: -1
            width: 64
            block_size: 4
            datawidth: 16
            metadata_storage_width: 60
            metadata_datawidth: 5
            metadata_block_size: 12

        - name: shared_glb
          class: storage
          subclass: smartbuffer_metadata
          required_actions: [read, write, update, leak]
          attributes:
            technology: -1
            data_storage_depth: 12800
            data_storage_width: 64
            block_size: 4
            n_banks: 1
            read_bandwidth: 16
            write_bandwidth: 16
            metadata_storage_depth: 0
            metadata_storage_width: 0
            metadata_block_size: 1
            metadata_datawidth: 0
            metadata_counter_width: 9
            decompression_supported: true  # from DRAM decompression
            compression_supported: true    # to DRAM compression

        - name: DummyBuffer[0..13]
          class: storage
          subclass: SRAM
          required_actions: [read, write, update, leak]
          attributes:
            technology: -1
            width: 16
            depth: 0
            num_banks: 1
            datawidth: 16
            entries: 0
            meshX: 14

      subtree:
        - name: PE[0..167]
          local:
            - name: ifmap_spad
              class: storage
              subclass: smartbuffer
              required_actions: [read, write, update, leak]
              attributes:
                technology: -1
                data_storage_depth: 12
                data_storage_width: 17 # actual data + 1bit mask
                n_banks: 1
                datawidth: 17
                read_bandwidth: 2
                write_bandwidth: 2
                meshX: 14

            - name: weights_spad
              class: storage
              subclass: smartbuffer
              required_actions: [read, write, update, leak]
              attributes:
                technology: -1
                data_storage_depth: 224
                data_storage_width: 16
                n_banks: 1
                datawidth: 16
                read_bandwidth: 2
                write_bandwidth: 2
                meshX: 14


            - name: psum_spad
              class: storage
              subclass: smartbuffer
              required_actions: [read, write, update, leak]
              attributes:
                technology: -1
                data_storage_depth: 24
                data_storage_width: 16
                n_banks: 1
                datawidth: 16
                meshX: 14

            - name: MACs
              class: compute
              subclass: mac
              required_actions: [read, write, update, leak]
              attributes:
                technology: -1
                meshX: 14
                datawidth: 16


# eyeriss v1 optimization features
#  - DRAM compression for Inputs and Weights
#  - gating on weights scratchpad
#  - note that the bitmask metadata associated with the ifmap scratchpad is not here because the metadata did not
#    allow ifmap accesses to be saved, which is what conventionally what metadata will help to achieve, so we model
#    it as an extra bit in the datawidth instead of as a metadata
sparse_optimizations:
  version: 0.4
  targets:
    - name: DRAM
      representation_format:
        data_spaces:
          - name: Inputs
            ranks:
              - format: UOP
                flattened_rankIDs: [ [R, S, P, C, M, N, Q] ]
              - format: RLE
                flattened_rankIDs: [ [R, S, P, C, M, N, Q] ]

          - name: Outputs
            ranks:
              - format: UOP
                flattened_rankIDs: [ [P, M, N, Q] ]
              - format: RLE
                flattened_rankIDs: [ [P, M, N, Q] ]

    - name: weights_spad
      action_optimization:
        - type: gating
          options:
            - target: Weights
              condition_on: [ Inputs ]

#
# The following constraints are limitations of the hardware architecture and dataflow
#

architecture_constraints:
  version: 0.4
  targets:
  # certain buffer only stores certain datatypes
  - target: psum_spad
    type: bypass
    bypass: [Inputs, Weights]
    keep: [Outputs]
  - target: weights_spad
    type: bypass
    bypass: [Inputs, Outputs]
    keep: [Weights]
  - target: ifmap_spad
    type: bypass
    bypass: [Weights, Outputs]
    keep: [Inputs]
  - target: DummyBuffer
    type: bypass
    bypass: [Inputs, Outputs, Weights]
  - target: shared_glb
    type: bypass
    bypass: [Weights]
    keep: [Inputs, Outputs]
  - target: DummyBuffer
    type: spatial
    split: 4
    permutation: NPQR SCM
    factors: P=1 Q=1 R=1 S=0 N=1
  # only allow fanout of M, Q out from glb
  - target: shared_glb
    type: spatial
    split: 7
    permutation: NCPRSQM
    factors: C=1 P=1 R=1 S=1 N=1
  # one ofmap position but of different output channels
  - target: psum_spad
    type: temporal
    permutation: NCPQRS M
    factors: C=1 R=1 S=1 P=1 Q=1 #N=1
  # row stationary -> 1 row at a time
  - target: weights_spad
    type: temporal
    permutation: NMPQS CR
    factors: M=1 P=1 Q=1 S=1 R=0 #N=1
  - target: ifmap_spad
    type: temporal
    permutation: NMCPQRS
    factors: M=1 C=1 P=1 Q=1 R=1 S=1 #N=1
  # enforce the hardware limit of the bypassing everything
  - target: DummyBuffer
    type: temporal
    factors: N=1 M=1 C=1 P=1 Q=1 R=1 S=1 


#
# The following constraints are not limitations of the hardware architecture and dataflow,
# but help limit the search space to speed up search
#

mapspace_constraints:
  version: 0.4
  targets:
    # intuitive optimization to reduce map space size
    # the factors of these are 1 anyways, so the order does not really matter
    - target: DummyBuffer
      type: temporal
      permutation: NMCPQRS
    # intuitive optimization for row stationary
    # -> process a row/col of the output before going to the next one
    - target: shared_glb
      type: temporal
      permutation: QRSC PNM
      factors: Q=1 R=1 S=1 P=0
    # intuitive optimization to reduce map space size
    - target: DRAM
      type: temporal
      permutation: RSP CMNQ
      factors: R=1 S=1 P=1
